from agente import AgenteOpenAIFunctions

agente = AgenteOpenAIFunctions()

query = """
VocÃª Ã© um assistente inteligente com acesso a duas ferramentas:

1ï¸âƒ£ DadosDeEstudante â€” busca dados de um estudante no CSV.
2ï¸âƒ£ PerfilAcademico â€” cria um perfil acadÃªmico a partir dos dados do estudante.

Sua tarefa Ã© decidir, de forma autÃ´noma, qual ferramenta usar (ou nenhuma)
para responder Ã  pergunta do usuÃ¡rio.

Pergunta: "Dentre todas as faculdades disponÃ­veis, quais Ana tem mais chance de entrar?"

"""

resposta = agente.llm_com_tools.invoke(query, tool_choice="auto")

print("ğŸ§  Resposta bruta do modelo:", resposta)

tool_calls = getattr(resposta, "tool_calls", [])

resultados = []

if not tool_calls:
    print("âŒ Nenhuma ferramenta foi chamada.")
else:
    print(f"ğŸ”§ {len(tool_calls)} ferramenta(s) chamada(s):")

    for call in tool_calls:
        nome_tool = call.get("name")
        args = call.get("args", {})

        print(f"\nğŸ“Œ Tool Call detectado:")
        print(f"   â€¢ Tool: {nome_tool}")
        print(f"   â€¢ Args: {args}")

        if nome_tool == "DadosDeEstudante":
            entrada = args.get("input", "")
            resultado = agente.dados_de_estudante.run(entrada)
            resultados.append(resultado)
            print("âœ… Resultado da ferramenta:", resultado)

        elif nome_tool == "PerfilAcademico":
            entrada = args.get("input", "")
            resultado = agente.perfil_academico.run(entrada)
            resultados.append(resultado)
            print("âœ… Resultado da ferramenta:", resultado)
        
        
        elif nome_tool == "DadosDeUniversidade":
            entrada = args.get("input", "")
            resultado = agente.dados_da_universidade.run(entrada)
            resultados.append(resultado)
            print("âœ… Resultado da ferramenta:", resultado)

    print("\nğŸ“š TODOS OS RESULTADOS:")
    for item in resultados:
        print("-", item)

    # ğŸ§© Etapa 2 â€” Reenvia resultados ao modelo para conclusÃ£o
    contexto = "\n".join(resultados)
    prompt_final = f"""
    Aqui estÃ£o os dados obtidos das ferramentas:

    {contexto}

    Agora responda Ã  pergunta original de forma completa e contextualizada:
    {query}
    """

    resposta_final = agente.llm.invoke(prompt_final)
    print("\nğŸ’¬ Resposta final do modelo:")
    print(resposta_final.content)
