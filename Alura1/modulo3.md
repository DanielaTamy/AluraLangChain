
# üìö M√≥dulo 3 - Conversa com Chat

## AULA 1 ‚Äì Simulando uma Intera√ß√£o de Chat sem Mem√≥ria

Nesta aula, aprendemos como simular uma conversa com um modelo de linguagem da Azure OpenAI utilizando LangChain, sem o uso de mem√≥ria de contexto. Cada pergunta √© enviada de forma independente, e o modelo responde sem lembrar das intera√ß√µes anteriores.

---

### ‚úÖ Instala√ß√£o

Certifique-se de instalar os pacotes necess√°rios:

```bash
pip install langchain-openai
pip install python-dotenv
```

---

### üß† Objetivo

- Simular uma conversa com m√∫ltiplas perguntas.
- Demonstrar que, sem mem√≥ria, o modelo n√£o mant√©m o contexto entre as mensagens.

---

### üß™ C√≥digo da Aula

```python
import os 
from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI

load_dotenv()

api_key = os.getenv("AZURE_OPENAI_KEY")
endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

if not api_key or not endpoint:
    raise ValueError("A chave da API ou o endpoint n√£o foram definidos no .env")

llm = AzureChatOpenAI(
    api_key=api_key,
    azure_endpoint=endpoint,
    azure_deployment="gpt-4o-mini",
    api_version="2024-05-01-preview",
    temperature=0.5
)

print("Chave e endpoint carregados com sucesso!")

lista_perguntas = [
    "Quero visitar um lugar do Brasil, famoso por praias e cultura. Pode sugerir?",
    "Qual a melhor √©poca do ano para ir?"
]

for uma_pergunta in lista_perguntas:
    resposta = llm.invoke(uma_pergunta)
    print(f"Usu√°rio: {uma_pergunta}")
    print(f"IA: {resposta.content}
")
```

---

### üîç Observa√ß√µes

- Cada chamada ao `llm.invoke()` √© independente.
- O modelo n√£o tem mem√≥ria entre as intera√ß√µes.
- Para manter o contexto entre mensagens, ser√° necess√°rio adicionar mem√≥ria nas pr√≥ximas aulas.

---

### üîê Configura√ß√£o do `.env`

```env
AZURE_OPENAI_KEY=your_azure_openai_key
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
```


## AULA 2 ‚Äì Simulando uma Conversa com Mem√≥ria

Nesta aula, aprendemos como simular uma conversa com mem√≥ria utilizando LangChain. Ao contr√°rio da aula anterior, agora o modelo consegue lembrar das intera√ß√µes anteriores dentro de uma mesma sess√£o.

---

### ‚úÖ Objetivo

- Utilizar `InMemoryChatMessageHistory` para manter o hist√≥rico da conversa.
- Criar uma cadeia com mem√≥ria usando `RunnableWithMessageHistory`.
- Simular uma conversa cont√≠nua com o modelo AzureChatOpenAI.

---

### üß† Componentes Utilizados

- `AzureChatOpenAI`: modelo de linguagem da Azure via LangChain.
- `ChatPromptTemplate`: estrutura de prompt com hist√≥rico e entrada do usu√°rio.
- `InMemoryChatMessageHistory`: armazena o hist√≥rico da conversa em mem√≥ria.
- `RunnableWithMessageHistory`: encapsula a cadeia com suporte a sess√µes de chat.

---

### üß™ Exemplo de C√≥digo

```python
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Configura√ß√£o do modelo e prompt
prompt_sugestao = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um guia de viagem especializado em destinos brasileiros. Apresente-se como Sr. Passeios."),
    ("placeholder", "{historico}"),
    ("human", "{query}"),
])

cadeia = prompt_sugestao | llm | StrOutputParser()

# Mem√≥ria por sess√£o
memoria = {}
def historico_por_sessao(sessao: str):
    if sessao not in memoria:
        memoria[sessao] = InMemoryChatMessageHistory()
    return memoria[sessao]

# Cadeia com mem√≥ria
cadeia_com_memoria = RunnableWithMessageHistory(
    runnable=cadeia,
    get_session_history=historico_por_sessao,
    input_messages_key="query",
    history_messages_key="historico"
)

# Simula√ß√£o de conversa
lista_perguntas = [
    "Quero visitar um lugar do Brasil, famoso por praias e cultura. Pode sugerir?",
    "Qual a melhor √©poca do ano para ir?"
]

for pergunta in lista_perguntas:
    resposta = cadeia_com_memoria.invoke({"query": pergunta}, config={"session_id": "aula_langchain_alura"})
    print(f"Usu√°rio: {pergunta}")
    print(f"IA: {resposta}
")
```

---

### üîç Observa√ß√µes

- A mem√≥ria √© mantida por sess√£o com `session_id`.
- O modelo consegue responder com base nas perguntas anteriores.
- Ideal para simular assistentes conversacionais com contexto.

---

### üîê Configura√ß√£o do `.env`

```env
AZURE_OPENAI_KEY=your_azure_openai_key
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
```
